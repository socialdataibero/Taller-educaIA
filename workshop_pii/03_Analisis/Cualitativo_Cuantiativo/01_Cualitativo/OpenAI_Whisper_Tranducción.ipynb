{"cells":[{"cell_type":"markdown","metadata":{"id":"v5hvo8QWN-a9"},"source":["# **Whisper**"]},{"cell_type":"code","source":["#@title 💻 Ejecutar esta celda para instalar las librerías.\n","\n","!pip install git+https://github.com/openai/whisper.git\n","!pip install ffmpeg\n","\n"],"metadata":{"id":"SJl7HJOeo0-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710443001845,"user_tz":360,"elapsed":124151,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}},"outputId":"ff8ce1e9-73a9-4e8b-aadc-291563c3a11c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-i4bgpwz1\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-i4bgpwz1\n","  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n","Collecting tiktoken (from openai-whisper==20231117)\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=2296d0309cd20a5ca97b57de3654414410467b636cc53053871639061c5682bf\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m7i4qtul/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n","Collecting ffmpeg\n","  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: ffmpeg\n","  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=8154406a3e4d1318f7f508c53978ce026903092bc2171c4342b5c745e466177e\n","  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n","Successfully built ffmpeg\n","Installing collected packages: ffmpeg\n","Successfully installed ffmpeg-1.4\n"]}]},{"cell_type":"code","source":["#@title 🗂️  Montar la carpeta Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDWVjtAK2PcE","executionInfo":{"status":"ok","timestamp":1710443157922,"user_tz":360,"elapsed":156081,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}},"outputId":"a7d2614a-fb29-4ac7-b516-ea3b5aaec6f6","cellView":"form"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#@title 🗂️ Colocar la ruta de nuestro directorio de trabajo\n","\n","import os\n","os.chdir(\"/content/drive/MyDrive/09_DCenter/08_Comunidades_de_Aprendizaje/Programas/01_Whisper/\")\n"],"metadata":{"id":"mkokx_Dl3ePG","executionInfo":{"status":"ok","timestamp":1710443158534,"user_tz":360,"elapsed":616,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#@title 🎼  Colocar el nombre de nuestro audio\n","audio = \"Mito_y_oralidad_en_la_tradicion_mesoamericana_1_1.mp3\""],"metadata":{"id":"_x5LGzGi8wA4","executionInfo":{"status":"ok","timestamp":1710443158535,"user_tz":360,"elapsed":2,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#@title 🎛️ Nombrar el directorio de salida de nuestras transcripciones\n","output_dir = \"transcript1_idioma\""],"metadata":{"id":"lz3SEEPh9WBp","executionInfo":{"status":"ok","timestamp":1710443158736,"user_tz":360,"elapsed":203,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#@title Ajustar algunos parámetros\n","model = \"tiny\"            # Otros modelos son: tiny, base, small, medium, large, large-v2, large-v3\n","device = \"cuda\"               # Otro valor es: cpu\n","fp16 = False                  # Otro valor es: True\n","verbose = False               # Otro valor es: True\n","no_speech_threshold = 0.5     # El rago de valores es: [0,1]\n","temperature = 0               # El rango de valores es: [0,1]\n","beam_size = 5                 # El rango de valores es: [1,10]"],"metadata":{"id":"XImaoP3TEAA_","executionInfo":{"status":"ok","timestamp":1710443158737,"user_tz":360,"elapsed":4,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#@title 👉  Selecciona el idioma a traducir\n","idioma = \"English\"  # otros idiomas 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Cantonese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Mandarin', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba')"],"metadata":{"id":"c9J0nqVxVcVA","executionInfo":{"status":"ok","timestamp":1710443158737,"user_tz":360,"elapsed":4,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["#@title 👉  Sólo traducir audio del idioma original a otro idioma. (X ► otro idioma)\n","\n","!whisper \"$audio\" --language \"$idioma\" --task transcribe --model \"$model\" --device \"$device\" --fp16 \"$fp16\" --verbose \"$verbose\" --output_dir \"$output_dir\" --no_speech_threshold \"$no_speech_threshold\" --temperature \"$temperature\" --beam_size \"$beam_size\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfQRafjTE8g5","executionInfo":{"status":"ok","timestamp":1710443169232,"user_tz":360,"elapsed":10498,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}},"outputId":"06b1e255-5d15-42b6-d7dd-f84d61cafd76"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["100%|█████████████████████████████████████| 72.1M/72.1M [00:01<00:00, 41.4MiB/s]\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/whisper\", line 8, in <module>\n","    sys.exit(cli())\n","  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 577, in cli\n","    model = load_model(model_name, device=device, download_root=model_dir)\n","  File \"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py\", line 146, in load_model\n","    checkpoint = torch.load(fp, map_location=device)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1026, in load\n","    return _load(opened_zipfile,\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1438, in _load\n","    result = unpickler.load()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1408, in persistent_load\n","    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1382, in load_tensor\n","    wrap_storage=restore_location(storage, location),\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1308, in restore_location\n","    return default_restore_location(storage, map_location)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 391, in default_restore_location\n","    result = fn(storage, location)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 266, in _cuda_deserialize\n","    device = validate_cuda_device(location)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 250, in validate_cuda_device\n","    raise RuntimeError('Attempting to deserialize object on a CUDA '\n","RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"d2iFWKDeUAW5","executionInfo":{"status":"ok","timestamp":1710443169232,"user_tz":360,"elapsed":4,"user":{"displayName":"wilant Gomari","userId":"02639880264189271112"}}},"execution_count":8,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}